# Resume Match - Project Overview

## ğŸ¯ Project Vision

Resume Match is an AI-powered platform that revolutionizes the resume analysis and job matching process. By leveraging local Large Language Models (LLMs) through Ollama, it provides real-time, privacy-focused feedback to help users optimize their resumes for specific job opportunities.

## ğŸ—ï¸ Technical Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/SSE    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Backend      â”‚
â”‚   (React)       â”‚                â”‚     (Go)        â”‚
â”‚   Port 3000     â”‚                â”‚   Port 8080     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â”‚ HTTP
                                              â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   ML Service    â”‚
                                    â”‚   (Python)      â”‚
                                    â”‚   Port 8000     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â”‚ HTTP
                                              â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚     Ollama      â”‚
                                    â”‚   (LLM)        â”‚
                                    â”‚   Port 11434    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Responsibilities

#### 1. Frontend (React + TypeScript)

- **Technology Stack**: React 18, Vite, Material-UI, TypeScript
- **Key Features**:
  - File upload interface with drag-and-drop support
  - Real-time streaming display using Server-Sent Events
  - Responsive design with Material-UI components
  - Form validation and error handling
- **State Management**: React hooks for local state
- **API Integration**: Fetch API with streaming support

#### 2. Backend (Go + Gin)

- **Technology Stack**: Go 1.21+, Gin framework, HTTP client
- **Key Features**:
  - File upload handling and validation
  - Request proxying to ML service
  - CORS configuration
  - Streaming response forwarding
- **Architecture**: RESTful API with middleware support
- **Performance**: Optimized for concurrent requests

#### 3. ML Service (Python + FastAPI)

- **Technology Stack**: Python 3.11+, FastAPI, LiteLLM, PyPDF2, python-docx
- **Key Features**:
  - Multi-format resume parsing (PDF, DOCX, TXT)
  - Skill extraction and analysis
  - LLM integration via LiteLLM
  - Streaming response generation
- **AI Integration**: OpenAI-compatible API to Ollama
- **File Processing**: Efficient text extraction and cleaning

#### 4. Ollama Service

- **Technology Stack**: Ollama with local model inference
- **Key Features**:
  - Local LLM processing for privacy
  - GPU acceleration support (Apple Metal)
  - Flash Attention optimization
  - Model management and switching
- **Performance**: Optimized for inference speed

## ğŸ”„ Data Flow

### 1. Resume Upload Process

```
User Upload â†’ Frontend â†’ Backend â†’ ML Service â†’ File Parsing â†’ Text Extraction
```

### 2. Analysis Process

```
Extracted Text â†’ Skill Analysis â†’ LLM Prompt â†’ Ollama â†’ Response Generation
```

### 3. Streaming Response

```
LLM Response â†’ ML Service â†’ Backend â†’ Frontend â†’ Real-time Display
```

## ğŸš€ Key Features Implementation

### Real-time Streaming

- **Technology**: Server-Sent Events (SSE)
- **Implementation**:
  - ML service generates streaming responses
  - Backend forwards streams to frontend
  - Frontend displays content in real-time
- **Benefits**: Immediate feedback, better user experience

### Multi-format Support

- **PDF**: PyPDF2 for text extraction
- **DOCX**: python-docx for structured content
- **TXT**: Direct text processing
- **Fallback**: Graceful error handling for unsupported formats

### Privacy-First Design

- **Local Processing**: All AI operations happen locally
- **No External APIs**: Complete data sovereignty
- **Secure Handling**: Proper file validation and cleanup

## ğŸ”§ Configuration Management

### Environment Variables

```bash
# Backend
ML_URL=http://ml:8000/api/score_file_stream

# ML Service
OLLAMA_BASE_URL=http://ollama:11434

# Ollama
OLLAMA_GPU_LAYERS=35
OLLAMA_FLASH_ATTENTION=true
OLLAMA_METAL=1
```

### Docker Configuration

- **Multi-stage builds** for optimized images
- **Service dependencies** and health checks
- **Volume mounts** for persistent data
- **Port mapping** for service communication

### Input Validation

- File type verification
- Size limits enforcement
- Content sanitization
- Malicious file detection

### Network Security

- CORS configuration
- Request rate limiting
- Secure file handling
- No external data transmission

### Privacy Protection

- Local data processing
- No logging of sensitive content
- Secure file cleanup
- User data isolation

## ğŸ” Monitoring and Debugging

### Logging Strategy

- **Backend**: Structured logging with levels
- **ML Service**: Detailed processing logs
- **Ollama**: Model inference logs
- **Frontend**: Error logging and user feedback

### Health Checks

- Service availability monitoring
- Dependency checking
- Performance metrics
- Error rate tracking
